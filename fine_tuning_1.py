# -*- coding: utf-8 -*-
"""fine_tuning_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XZyyyZ4agrS8DHAXKlIFFq9y9n29OXow
"""

!pip install transformers datasets pymongo[srv] accelerate evaluate sacrebleu
!pip install -U transformers

from pymongo import MongoClient
from pymongo.server_api import ServerApi
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

# -------------------------
# MongoDB Connection
# -------------------------
username = "patilchaitra612"
password = "12345"
uri = f"mongodb+srv://{username}:{password}@cluster1.oxcvzfp.mongodb.net/?retryWrites=true&w=majority&appName=Cluster1"

client = MongoClient(uri, server_api=ServerApi('1'))
db = client["hindi_translation_DB"]
collection = db["translations_collection"]

# Load all data
data = list(collection.find({}, {"_id": 0}))
df = pd.DataFrame(data)
print("Data Preview:")
print(df.head())

# -------------------------
# Model Training Function
# -------------------------
def train_translation_model(df, target_lang_code, target_col_name, save_dir):


    # Keep only needed columns
    df_lang = df[['Hindi', target_col_name]].dropna()

    # Remove empty strings
    df_lang = df_lang[(df_lang['Hindi'].str.strip() != "") & (df_lang[target_col_name].str.strip() != "")]

    # Rename for Hugging Face dataset
    df_lang = df_lang.rename(columns={'Hindi': 'source', target_col_name: 'target'})

    # Convert to Hugging Face Dataset
    dataset = Dataset.from_pandas(df_lang)
    dataset = dataset.train_test_split(test_size=0.1)
    train_dataset = dataset["train"]
    val_dataset = dataset["test"]

    # Load NLLB tokenizer and model
    model_name = "facebook/nllb-200-distilled-600M"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    # Preprocessing function
    def preprocess_function(examples):
        inputs = examples["source"]
        targets = examples["target"]

        tokenizer.src_lang = "hin_Deva"
        tokenizer.tgt_lang = target_lang_code

        model_inputs = tokenizer(inputs, max_length=128, truncation=True)
        labels = tokenizer(text_target=targets, max_length=128, truncation=True)

        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized_train = train_dataset.map(preprocess_function, batched=True, batch_size=4, remove_columns=train_dataset.column_names)
    tokenized_val = val_dataset.map(preprocess_function, batched=True, batch_size=4, remove_columns=val_dataset.column_names)

    # Training setup (compatible with old transformers)
    training_args = Seq2SeqTrainingArguments(
        output_dir=save_dir,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        save_total_limit=1,
        learning_rate=5e-5
    )

    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    # Train
    trainer.train()

    # Save model & tokenizer
    trainer.save_model(save_dir)
    tokenizer.save_pretrained(save_dir)
    print(f"✅ Model saved at {save_dir}")

# -------------------------
# Train Hindi → English
# -------------------------
train_translation_model(
    df,
    target_lang_code="eng_Latn",
    target_col_name="Translation (English)",
    save_dir="nllb-hin-en-finetuned"
)

# -------------------------
# Train Hindi → Kannada
# -------------------------
train_translation_model(
    df,
    target_lang_code="kan_Knda",
    target_col_name="Translation (Kannada)",
    save_dir="nllb-hin-kn-finetuned"
)

# -------------------------
# Train Hindi → Tamil
# -------------------------
train_translation_model(
    df,
    target_lang_code="tam_Taml",
    target_col_name="Translation (Tamil)",
    save_dir="nllb-hin-ta-finetuned"
)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

def load_translator(model_path, src_lang_code, tgt_lang_code):
    """Load a translation pipeline for a given source and target language."""
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

    translator_pipeline = pipeline(
        "translation",
        model=model,
        tokenizer=tokenizer,
        src_lang=src_lang_code,
        tgt_lang=tgt_lang_code,
        max_length=200
    )
    return translator_pipeline

# -------------------------
# Model paths for each target
# (use your own fine-tuned model folder paths)
# -------------------------
models = {
    "kn": {"path": "nllb-hin-kn-finetuned", "tgt": "kan_Knda"},
    "en": {"path": "nllb-hin-en-finetuned", "tgt": "eng_Latn"},
    "ta": {"path": "nllb-hin-ta-finetuned", "tgt": "tam_Taml"}
}

# -------------------------
# Translate function
# -------------------------
def translate_text(hindi_text, target_lang):
    if target_lang not in models:
        raise ValueError(f"Target lang '{target_lang}' not supported. Choose from {list(models.keys())}")

    model_info = models[target_lang]
    translator = load_translator(model_info["path"], "hin_Deva", model_info["tgt"])
    translation = translator(hindi_text)
    return translation[0]['translation_text']

# Example usage
# -------------------------
hindi_text = "भारतीय सेना के बारे में जानकारी इस प्रकार है: भारतीय सेना, भारतीय सशस्त्र बलों का एक प्रमुख अंग है, जो देश की सुरक्षा और रक्षा में महत्वपूर्ण भूमिका निभाती है। यह दुनिया की दूसरी सबसे बड़ी सैन्य शक्ति है और दुनिया की सबसे बड़ी स्वयंसेवी सेना है. "

print("Hindi:", hindi_text)
print("Kannada:", translate_text(hindi_text, "kn"))
print("English:", translate_text(hindi_text, "en"))
print("Tamil:", translate_text(hindi_text, "ta"))

from google.colab import drive
drive.mount('/content/drive')